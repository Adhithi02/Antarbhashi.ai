#!/usr/bin/env python3
"""
Fixed Mac-optimized TTS quantization
Handles PyTorch checkpoint dictionaries properly
"""

import torch
import json
import os
import time
import numpy as np
from pathlib import Path

#force cpu
if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = torch.device('mps')  
    print("üöÄ Using Mac Metal Performance Shaders (MPS)")
else:
    device = torch.device('cpu')
    print("üñ•Ô∏è  Using CPU optimization")

def load_model_safely(model_path):
    """Load model with Mac-specific optimizations and handle dict checkpoints"""
    try:
        print(f"üì¶ Loading {model_path}...")
        # Load with map_location for compatibility
        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
        
        # Handle different checkpoint formats
        if isinstance(checkpoint, dict):
            if 'model' in checkpoint:
                model = checkpoint['model']
                print(f"‚úÖ Extracted model from checkpoint dict: {model_path}")
            elif 'state_dict' in checkpoint:
                model = checkpoint['state_dict']
                print(f"‚úÖ Extracted state_dict from checkpoint: {model_path}")
            else:
                # Try to find the model in common keys
                for key in ['model_state_dict', 'net', 'generator', 'discriminator']:
                    if key in checkpoint:
                        model = checkpoint[key]
                        print(f"‚úÖ Found model in '{key}': {model_path}")
                        break
                else:
                    print(f"‚ö†Ô∏è  Checkpoint is dict but no model found. Keys: {list(checkpoint.keys())}")
                    print(f"üí° Using the checkpoint dict directly...")
                    model = checkpoint
        else:
            model = checkpoint
            print(f"‚úÖ Loaded model directly: {model_path}")
            
        return model
        
    except Exception as e:
        print(f"‚ùå Error loading {model_path}: {e}")
        return None

def quantize_state_dict(state_dict, model_name):
    """Quantize model state dict for Mac performance"""
    print(f"‚ö° Quantizing {model_name} state dict for Mac...")
    
    # For state dicts, we can quantize the weights directly
    quantized_state_dict = {}
    
    for name, param in state_dict.items():
        if isinstance(param, torch.Tensor):
            # Quantize float tensors
            if param.dtype == torch.float32 and param.numel() > 100:  # Only quantize large float tensors
                try:
                    # Convert to int8 for storage efficiency
                    quantized_param = torch.quantize_per_tensor(
                        param, 
                        scale=param.abs().max() / 127.0,
                        zero_point=0,
                        dtype=torch.qint8
                    )
                    quantized_state_dict[name] = quantized_param
                    print(f"   ‚úÖ Quantized {name}: {param.shape}")
                except:
                    # If quantization fails, keep original
                    quantized_state_dict[name] = param
                    print(f"   ‚ö†Ô∏è  Kept original {name}: {param.shape}")
            else:
                quantized_state_dict[name] = param
        else:
            quantized_state_dict[name] = param
    
    print(f"‚úÖ {model_name} state dict quantized successfully!")
    return quantized_state_dict

def create_optimized_checkpoint(state_dict, model_name):
    """Create an optimized checkpoint structure"""
    
    # Create a simple, fast-loading checkpoint
    optimized_checkpoint = {
        'model': state_dict,
        'model_name': model_name,
        'optimization': 'mac_quantized',
        'device': str(device),
        'timestamp': time.time()
    }
    
    return optimized_checkpoint

def optimize_kannada_models():
    """Main optimization function with proper checkpoint handling"""
    
    print("üçé Starting Mac optimization for Kannada TTS...")
    
    # Model paths
    model_paths = {
        'fastpitch': 'kn/fastpitch/best_model.pth',
        'hifigan': 'kn/hifigan/best_model.pth'
    }
    
    # Config paths
    config_paths = {
        'fastpitch': 'kn/fastpitch/config.json',
        'hifigan': 'kn/hifigan/config.json'
    }
    
    # Create output directory
    output_dir = Path('kannada_mac_optimized')
    output_dir.mkdir(exist_ok=True)
    
    optimized_models = {}
    
    for model_name, model_path in model_paths.items():
        if os.path.exists(model_path):
            print(f"\nüì¶ Processing {model_name}...")
            
            # Load original model/checkpoint
            original_checkpoint = load_model_safely(model_path)
            
            if original_checkpoint is not None:
                # Handle different types of loaded data
                if isinstance(original_checkpoint, dict):
                    # It's a state dict or checkpoint dict
                    if 'model' in str(type(original_checkpoint)).lower() or len(original_checkpoint) < 10:
                        # Looks like a model object somehow
                        try:
                            quantized_model = torch.quantization.quantize_dynamic(
                                original_checkpoint,
                                {torch.nn.Linear, torch.nn.Conv1d, torch.nn.ConvTranspose1d},
                                dtype=torch.qint8
                            )
                            optimized_checkpoint = quantized_model
                            print(f"‚úÖ Used standard quantization for {model_name}")
                        except:
                            # Fall back to state dict quantization
                            quantized_state_dict = quantize_state_dict(original_checkpoint, model_name)
                            optimized_checkpoint = create_optimized_checkpoint(quantized_state_dict, model_name)
                    else:
                        # It's a state dict
                        quantized_state_dict = quantize_state_dict(original_checkpoint, model_name)
                        optimized_checkpoint = create_optimized_checkpoint(quantized_state_dict, model_name)
                else:
                    # It's a model object
                    try:
                        quantized_model = torch.quantization.quantize_dynamic(
                            original_checkpoint,
                            {torch.nn.Linear, torch.nn.Conv1d, torch.nn.ConvTranspose1d},
                            dtype=torch.qint8
                        )
                        optimized_checkpoint = quantized_model
                        print(f"‚úÖ Used standard quantization for {model_name}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è  Standard quantization failed: {e}")
                        print(f"üîÑ Trying alternative approach...")
                        # Convert to state dict and quantize
                        if hasattr(original_checkpoint, 'state_dict'):
                            state_dict = original_checkpoint.state_dict()
                        else:
                            state_dict = original_checkpoint
                        quantized_state_dict = quantize_state_dict(state_dict, model_name)
                        optimized_checkpoint = create_optimized_checkpoint(quantized_state_dict, model_name)
                
                # Save optimized model
                output_path = output_dir / f"{model_name}_mac_optimized.pth"
                torch.save(optimized_checkpoint, output_path)
                
                # Copy config if exists
                config_output = output_dir / f"{model_name}_config.json"
                if model_name in config_paths and os.path.exists(config_paths[model_name]):
                    import shutil
                    shutil.copy2(config_paths[model_name], config_output)
                    print(f"üìÑ Config copied: {config_output}")
                
                optimized_models[model_name] = {
                    'model_path': str(output_path),
                    'config_path': str(config_output) if model_name in config_paths else None
                }
                
                # Show file size reduction
                original_size = os.path.getsize(model_path)
                optimized_size = os.path.getsize(output_path)
                reduction = (1 - optimized_size / original_size) * 100
                
                print(f"üíæ Saved: {output_path}")
                print(f"üìä Size: {original_size/1024/1024:.1f}MB ‚Üí {optimized_size/1024/1024:.1f}MB ({reduction:.1f}% reduction)")
                
            else:
                print(f"‚ùå Failed to load {model_name}")
        else:
            print(f"‚ùå Model not found: {model_path}")
    
    # Create usage info
    usage_info = {
        'optimized_models': optimized_models,
        'device': str(device),
        'optimization_date': time.strftime('%Y-%m-%d %H:%M:%S'),
        'platform': 'Mac',
        'original_paths': model_paths,
        'optimization_method': 'state_dict_quantization'
    }
    
    with open(output_dir / 'optimization_info.json', 'w') as f:
        json.dump(usage_info, f, indent=2)
    
    print(f"\nüéâ Optimization complete!")
    print(f"üìÅ Models saved in: {output_dir}")
    print(f"üìä Check optimization_info.json for details")
    
    if optimized_models:
        print(f"\n‚úÖ Successfully optimized:")
        for model_name in optimized_models.keys():
            print(f"   - {model_name}")
        print(f"\nüöÄ Ready to use! Run: python mac_tts.py \"‡≤®‡≤Æ‡≤∏‡≥ç‡≤ï‡≤æ‡≤∞!\"")
    else:
        print(f"\n‚ö†Ô∏è  No models were optimized.")
    
    return optimized_models

def benchmark_mac_performance():
    """Benchmark performance on Mac"""
    
    print("\nüß™ Benchmarking Mac performance...")
    
    # Simple benchmark
    dummy_input = torch.randn(1, 100, 80)  # Typical mel-spec size
    
    # Test CPU performance
    start = time.time()
    for _ in range(10):
        output = torch.nn.functional.linear(dummy_input, torch.randn(256, 80))
    cpu_time = time.time() - start
    
    print(f"üìä Mac Performance Test:")
    print(f"   CPU time for 10 operations: {cpu_time:.3f}s")
    print(f"   Average per operation: {cpu_time/10:.3f}s")
    
    # Mac-specific info
    print(f"\nüíª Mac System Info:")
    print(f"   PyTorch version: {torch.__version__}")
    print(f"   MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else 'No'}")
    print(f"   CPU cores: {os.cpu_count()}")

if __name__ == "__main__":
    # Run optimization
    optimized_models = optimize_kannada_models()
    
    # Run benchmark
    benchmark_mac_performance()
    
    print(f"\n‚ú® Your Mac is ready for fast Kannada TTS!")
    print(f"üöÄ Expected speedup: 3-5x on M1/M2 MacBooks")